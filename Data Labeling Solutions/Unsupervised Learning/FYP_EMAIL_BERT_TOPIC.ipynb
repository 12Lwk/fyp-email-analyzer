{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "email_data = pd.read_csv('/content/drive/MyDrive/FYPDataset/further_cleaned_enron_dataset.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpdzkKecNRbf",
        "outputId": "f4cf6e40-033b-44c8-fe7b-c4a6dbc11b32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from rake_nltk import Rake\n",
        "from yake import KeywordExtractor\n",
        "from keybert import KeyBERT"
      ],
      "metadata": {
        "id": "d1yrbTjnQr9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "r = Rake()  # Initialize RAKE\n",
        "kw_extractor = KeywordExtractor()  # Initialize YAKE\n",
        "kw_model = KeyBERT()  # Initialize KeyBERT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhI8uUg7TmQa",
        "outputId": "f6fef2b3-52ba-4fd3-ef0e-0ea06ee36ed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [text.split() if isinstance(text, str) and text else [] for text in email_data['Cleaned_Message']]\n",
        "dictionary = corpora.Dictionary(corpus)\n",
        "doc_term_matrix = [dictionary.doc2bow(doc) for doc in corpus]"
      ],
      "metadata": {
        "id": "jt6eq53AUTdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_topics = 10  # Experiment with this number\n",
        "lda_model = gensim.models.LdaModel(doc_term_matrix, num_topics=num_topics, id2word=dictionary)\n",
        "\n",
        "for topic_id in range(num_topics):\n",
        "    top_words = lda_model.show_topic(topic_id, topn=10)\n",
        "    print(f\"Topic {topic_id}: {', '.join([word for word, prob in top_words])}\")\n",
        "\n",
        "topic_distributions = [lda_model.get_document_topics(bow) for bow in doc_term_matrix]\n",
        "email_data['topic_distribution'] = topic_distributions\n",
        "\n",
        "def convert_topic_dist(topic_dist):\n",
        "    topic_vec = [0] * num_topics\n",
        "    for topic_id, prob in topic_dist:\n",
        "        topic_vec[topic_id] = prob\n",
        "    return topic_vec\n",
        "\n",
        "email_data['topic_vector'] = email_data['topic_distribution'].apply(convert_topic_dist)\n",
        "topic_matrix = pd.DataFrame(email_data['topic_vector'].tolist())\n",
        "email_data = pd.concat([email_data, topic_matrix], axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TezJtJC9XKyA",
        "outputId": "9c8c6b8e-e198-4e9c-f1fd-5f287945269a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0: image, day, go, know, time, week, want, like, think, good\n",
            "Topic 1: url, travel, tx, new, fare, click, hotel, houston, city, n\n",
            "Topic 2: final, market, ferc, file, customer, issue, transmission, commission, request, order\n",
            "Topic 3: gas, capacity, day, d, pipeline, volume, contract, point, delivery, storage\n",
            "Topic 4: email, subject, pm, cc, forward, enron, hou, ect, j, john\n",
            "Topic 5: schedule, date, pm, hour, time, meeting, start, am, description, calendar\n",
            "Topic 6: e, mail, email, receive, message, url, information, click, send, database\n",
            "Topic 7: deal, enron, agreement, change, attach, trade, fax, transaction, credit, need\n",
            "Topic 8: energy, company, power, market, price, say, california, year, news, state\n",
            "Topic 9: enron, time, need, work, know, group, meeting, week, business, like\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Feature Engineering (Beyond Topics)\n",
        "# Sentiment Analysis\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import string  # Import the string module\n",
        "import textstat\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "email_data['message_sentiment'] = email_data['Cleaned_Message'].apply(lambda text: analyzer.polarity_scores(text)['compound'] if isinstance(text, str) else 0)\n",
        "email_data['subject_sentiment'] = email_data['Clean_Subject'].apply(lambda text: analyzer.polarity_scores(text)['compound'] if isinstance(text, str) else 0)\n",
        "\n",
        "\n",
        "# Email Length\n",
        "email_data['message_length'] = email_data['Cleaned_Message'].str.len()\n",
        "email_data['subject_length'] = email_data['Clean_Subject'].str.len()\n",
        "\n",
        "# Punctuation Count\n",
        "def count_punctuation(text):\n",
        "    if isinstance(text, str):\n",
        "      count = 0\n",
        "      for char in text:\n",
        "          if char in string.punctuation:  # Now 'string' is recognized\n",
        "              count += 1\n",
        "      return count\n",
        "    return 0\n",
        "\n",
        "email_data['message_punctuation_count'] = email_data['Cleaned_Message'].apply(count_punctuation)\n",
        "email_data['subject_punctuation_count'] = email_data['Clean_Subject'].apply(count_punctuation)\n",
        "\n",
        "# Readability Metrics\n",
        "email_data['message_readability'] = email_data['Cleaned_Message'].apply(lambda text: textstat.flesch_kincaid_grade(text) if isinstance(text, str) else 0)\n",
        "email_data['subject_readability'] = email_data['Clean_Subject'].apply(lambda text: textstat.flesch_kincaid_grade(text) if isinstance(text, str) else 0)\n",
        "\n",
        "# Sender Domain\n",
        "def get_domain(email):\n",
        "    if isinstance(email, str):\n",
        "      try:\n",
        "          return email.split('@')[1]\n",
        "      except:\n",
        "          return None\n",
        "    return None\n",
        "\n",
        "email_data['sender_domain'] = email_data['From'].apply(get_domain)\n",
        "\n",
        "# Time Features (Handle potential errors)\n",
        "try:\n",
        "    email_data['hour_of_day'] = pd.to_datetime(email_data['Date & Time']).dt.hour\n",
        "    email_data['day_of_week'] = pd.to_datetime(email_data['Date & Time']).dt.day_name()\n",
        "except (TypeError, ValueError) as e:\n",
        "    print(f\"Error processing 'Date & Time': {e}\")\n",
        "    email_data['hour_of_day'] = None\n",
        "    email_data['day_of_week'] = None"
      ],
      "metadata": {
        "id": "ZqTeuhw-Xa89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "email_data.to_csv('/content/drive/MyDrive/FYPDataset/email_data_with_topics_and_features.csv', index=False)"
      ],
      "metadata": {
        "id": "bQvz_YQYe9D0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}